{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adding_metadata_to_nwb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N679irLgbkwH"
      },
      "source": [
        "# NWB HACKATHON 2020: Injecting Metadata into IPFX converted NWB(s)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf3pvOZgb9GB"
      },
      "source": [
        "First, we have to install some dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SacWJrOCPVMH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6904f38-dd9e-484e-aced-ddb4e6d2a77b"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!git clone https://github.com/smestern/ipfx.git\n",
        "!git clone https://github.com/smestern/example-abf-files.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ipfx' already exists and is not an empty directory.\n",
            "fatal: destination path 'example-abf-files' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_NWkqZkQKv_"
      },
      "source": [
        "!apt-get install -qq /content/ipfx\n",
        "!pip uninstall statsmodels -y\n",
        "!pip uninstall tables -y\n",
        "!pip install statsmodels==0.9.0\n",
        "!pip install tables==3.5.1\n",
        "!pip install /content/ipfx --log /content/log.txt\n",
        "!pip install pynwb\n",
        "!pip install nwbwidgets\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UE1yMLhMOGg"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import h5py\n",
        "import argparse\n",
        "import logging\n",
        "import pynwb\n",
        "import json\n",
        "log = logging.getLogger(__name__)\n",
        "import pyabf\n",
        "from ipfx.x_to_nwb.ABFConverter import ABFConverter\n",
        "from ipfx.x_to_nwb.DatConverter import DatConverter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from hdmf.utils import docval, popargs\n",
        "from pynwb import NWBFile, register_class, load_namespaces, NWBHDF5IO, CORE_NAMESPACE, get_class\n",
        "from pynwb.spec import NWBNamespaceBuilder, NWBGroupSpec, NWBAttributeSpec\n",
        "from pynwb.file import LabMetaData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31toDT5LGsnJ"
      },
      "source": [
        "# Adding metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_vEhtOdEXT"
      },
      "source": [
        "The above commands downloads a few example abf files to be used in this tutorial. Additionally, it downloads & installs my (slightly modified) version of Allen Institute's IPFX. From here we will attempt to organize and build the ABFs into nwbs using metadata from in jsons.\n",
        "\n",
        "Adding meta data to the NWB presents a number of challenges. Notably, as of now ABFconverter does not utilize additional metadata included in the json. For example, including 'session description' in a json will not be written as the session description in the NWB. Although [pull request 400](https://github.com/AllenInstitute/ipfx/pull/400) attempts to fix this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2sZ6V-aWuJ1"
      },
      "source": [
        "## Step 1 - Define your Metadata in a json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOH6hFavK2Mk"
      },
      "source": [
        "Much like MCC settings pulled earlier. The script is designed to utilize json files with metadata that may not be included at the time of conversion.\n",
        "\n",
        "If you want overwrite a specific value, the json dictionary should be organize exactly like the organization of the NWB file. For example, if you want to overwrite the gain for sweep 1. In the NWB this is located at\n",
        "\n",
        "-|Acquisition (group)  \n",
        "---|index_000 (group)  \n",
        "-----> gain: value  \n",
        "So we include the following field in the json file:\n",
        "\n",
        "\n",
        "```\n",
        "\"acquisition\": {\n",
        "    \"index_000\": {\n",
        "      \"gain\": 555\n",
        "    }\n",
        "  }\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EzpDChiLyM6"
      },
      "source": [
        "I have prebuilt some metadata into the json file included:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSaLIyiAH7Uf"
      },
      "source": [
        "json_path = \"//content//example-abf-files//mcc-settings.json\"\n",
        "def loadJSON(filename):\n",
        "      with open(filename) as fh:\n",
        "                return json.load(fh)\n",
        "meta_data = loadJSON(json_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbAFw0ChKwqq"
      },
      "source": [
        "## Step 2 - Inject the metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2_yFofQdCRu"
      },
      "source": [
        "Here we will use the previously built NWB as an example. Lets load the NWB and inspect the gain and session description:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9oUQn9DR2O7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2bbb46e7-7784-4012-9b97-0164e811c5de"
      },
      "source": [
        "file = \"//content//example-abf-files//M10_SA_A1_C07.nwb\"\n",
        "with h5py.File(file,  \"r\") as nwb:\n",
        "  print(F\"gain: {nwb['acquisition']['index_000']['gain'][()]}\")\n",
        "  print(F\"sess desc: {nwb['session_description'][()]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gain: 555.0\n",
            "sess desc: TEST DESC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDudmR-HTcqp"
      },
      "source": [
        "The following function allows you to pass both a nwb file and dict of metadata. It will then inject the meta data into the file. Note that pyNWB and the nwb team seem to oppose to overwriting and/or deleting data, so this script has to go about this in a hack-y way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "416p8WZW3tja"
      },
      "source": [
        "def confirm_metadata(file, mjson, meta_field=True):\n",
        "    \"\"\"\n",
        "    Function Takes an input NWB, and INPUT json file(s). Checks to see if keys within the NWB that match JSON keys have matching content, if not, overwrites. Sometimes metadata\n",
        "    added to the json file is ignored by the ABFCONVERTER. Adds novel metadata keys as NWB extensions safely using pyNWB. Overwrites using h5py.\n",
        "    Takes:\n",
        "    file: path to NWB (hdf5) file,\n",
        "    mjson: path to json file to be injected into file.\n",
        "    meta_field: If true, all novel data is filed under a new group in the NWB file titled 'metadata' otherwise metadata is placed in base group\n",
        "    returns:\n",
        "    file: path to NWB file.\n",
        "    \"\"\"\n",
        "    def loadJSON(filename):\n",
        "        if isinstance(filename, (list, np.ndarray)):\n",
        "            full_dict = {}\n",
        "            for js in filename:\n",
        "                with open(js) as fh:\n",
        "                     full_dict.update(json.load(fh))\n",
        "            return full_dict\n",
        "        else:\n",
        "           with open(filename) as fh:\n",
        "                return json.load(fh)\n",
        "    def _h5_merge(dict1, dict2):\n",
        "        ''' Recursively merges the input'''\n",
        "\n",
        "        result = dict1\n",
        "\n",
        "        for key, value in dict2.items():\n",
        "            if isinstance(value, collections.Mapping):\n",
        "                _h5_merge(result.get(key, {}), value)\n",
        "            else:\n",
        "                result[key][...] = dict2[key]\n",
        "\n",
        "        return result\n",
        "    def dict_to_list(dict1):\n",
        "        list = []\n",
        "        for key, value in dict1.items():\n",
        "            if isinstance(value, (dict)):\n",
        "                list.append(dict_to_list(value))\n",
        "            else:\n",
        "                list.append((str(key), str(value)))\n",
        "        return list\n",
        "\n",
        "    metadata = loadJSON(mjson)\n",
        "    with h5py.File(file,  \"r+\") as f: ##Has to be opened with h5py as pyNWB does not support overwrite\n",
        "        NWB_f = f\n",
        "        nwb_keys = list(NWB_f.keys())\n",
        "        meta_keys = list(metadata.keys())\n",
        "        overlap_keys = np.intersect1d(nwb_keys,meta_keys) ##Look for overlapping and overwrite\n",
        "        for key in overlap_keys:\n",
        "            if isinstance(metadata[key], dict):\n",
        "                d = _h5_merge(f[key], metadata[key]) ## if its a dict instance, we begin the process of merging\n",
        "                ##recursively \n",
        "            else:\n",
        "                f[key][...] = metadata[key] ## Otherwise just overwrite. \n",
        "        novel_keys = np.setdiff1d(meta_keys, overlap_keys) ##Grab the novel keys for later\n",
        "    n_metadata = {key: metadata[key] for key in novel_keys}\n",
        "    ##Now close the nwb and open with pynwb\n",
        "    if False:\n",
        "      ##This section is not working in colab so avoid\n",
        "      with pynwb.NWBHDF5IO(file,  mode=\"r+\") as f_io:\n",
        "          ### Now add novel data using pynwb in a way thats a lot less brute force, and way more nwb friendly\n",
        "          f = f_io.read()\n",
        "          NWB_f = f\n",
        "          if False:\n",
        "              ##Currently not working \n",
        "              ##Class is compiled but attributes are not written to files\n",
        "              meta_class = build_settings(n_metadata)\n",
        "              test = get_class('MetaData', 'NHP')\n",
        "\n",
        "              nwb_meta = test(name='meta', experiment_id=int(12), test='test')\n",
        "              NWB_f.add_lab_meta_data(nwb_meta)\n",
        "          else:\n",
        "              ## For now just dump into scratch ##Goes against NWB Conventions however\n",
        "              for key, value in n_metadata.items():\n",
        "                  if isinstance(value, dict):\n",
        "                      cont = dict_to_list(value)\n",
        "                      for x in cont:\n",
        "                        NWB_f.add_scratch([x[1]], name=str(x[0]), notes=str(key))\n",
        "                  else:\n",
        "                      NWB_f.add_scratch([value], name=str(key), notes=\"null\")\n",
        "          f_io.write(NWB_f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvL7Tj6sXG9m"
      },
      "source": [
        "Now we try it and evaluate if it works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXxY9BC205r9"
      },
      "source": [
        "#try it\n",
        "confirm_metadata(file, json_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khzFSuxxXQCx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "56eb32a6-cb5d-449a-98fb-7c3731c061c7"
      },
      "source": [
        "file = \"//content//example-abf-files//M10_SA_A1_C07.nwb\"\n",
        "with h5py.File(file,  \"r\") as nwb:\n",
        "  print(F\"gain: {nwb['acquisition']['index_000']['gain'][()]}\")\n",
        "  print(F\"sess desc: {nwb['session_description'][()]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gain: 555.0\n",
            "sess desc: TEST DESC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKwif9RAYqol"
      },
      "source": [
        "\n",
        "You can see that the data was overwritten with the json file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uapeu2nCdS5J"
      },
      "source": [
        "## Step 3 - Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXYT4_VId8t5"
      },
      "source": [
        "Ideally we would utilize injection like this at the time of conversion. For our pratices we can call this immediatly after converting the ABF(s). Here we use the unmodified allen convert function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpBoHD_PeY7O"
      },
      "source": [
        "def convert(inFileOrFolder, overwrite=False, fileType=None, outputMetadata=False, outputFeedbackChannel=False, multipleGroupsPerFile=False, compression=True):\n",
        "    \"\"\"\n",
        "    Convert the given file to a NeuroDataWithoutBorders file using pynwb\n",
        "\n",
        "    Supported fileformats:\n",
        "        - ABF v2 files created by Clampex\n",
        "        - DAT files created by Patchmaster v2x90\n",
        "\n",
        "    :param inFileOrFolder: path to a file or folder\n",
        "    :param overwrite: overwrite output file, defaults to `False`\n",
        "    :param fileType: file type to be converted, must be passed iff `inFileOrFolder` refers to a folder\n",
        "    :param outputMetadata: output metadata of the file, helpful for debugging\n",
        "    :param outputFeedbackChannel: Output ADC data which stems from stimulus feedback channels (ignored for DAT files)\n",
        "    :param multipleGroupsPerFile: Write all Groups in the DAT file into one NWB\n",
        "                                  file. By default we create one NWB per Group (ignored for ABF files).\n",
        "    :param compression: Toggle compression for HDF5 datasets\n",
        "\n",
        "    :return: path of the created NWB file\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(inFileOrFolder):\n",
        "        raise ValueError(f\"The file {inFileOrFolder} does not exist.\")\n",
        "\n",
        "    if os.path.isfile(inFileOrFolder):\n",
        "        root, ext = os.path.splitext(inFileOrFolder)\n",
        "    if os.path.isdir(inFileOrFolder):\n",
        "        if not fileType:\n",
        "            raise ValueError(\"Missing fileType when passing a folder\")\n",
        "\n",
        "        inFileOrFolder = os.path.normpath(inFileOrFolder)\n",
        "        inFileOrFolder = os.path.realpath(inFileOrFolder)\n",
        "\n",
        "        ext = fileType\n",
        "        root = os.path.join(inFileOrFolder, \"..\",\n",
        "                            os.path.basename(inFileOrFolder))\n",
        "\n",
        "    outFile = root + \".nwb\"\n",
        "\n",
        "    if not outputMetadata and os.path.exists(outFile):\n",
        "        if overwrite:\n",
        "            os.remove(outFile)\n",
        "        else:\n",
        "            raise ValueError(f\"The output file {outFile} does already exist.\")\n",
        "\n",
        "    if ext == \".abf\":\n",
        "        if outputMetadata:\n",
        "            ABFConverter.outputMetadata(inFileOrFolder)\n",
        "        else:\n",
        "            ABFConverter(inFileOrFolder, outFile, outputFeedbackChannel=outputFeedbackChannel, compression=compression)\n",
        "    elif ext == \".dat\":\n",
        "        if outputMetadata:\n",
        "            DatConverter.outputMetadata(inFileOrFolder)\n",
        "        else:\n",
        "            DatConverter(inFileOrFolder, outFile, multipleGroupsPerFile=multipleGroupsPerFile, compression=compression)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"The extension {ext} is currently not supported.\")\n",
        "\n",
        "    return outFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6-fkn24fglk"
      },
      "source": [
        "Now we bulk convert again. Note this is from [this](https://github.com/smestern/ipfx/blob/master/ipfx/bin/run_bulk_to_nwb_conversion.py) script which allows you to call bulk conversion from the command line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvjwMKQheHcV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "ee40c2e0-edfe-4265-d8fb-81a1c81127cd"
      },
      "source": [
        "\n",
        "bmeta=True\n",
        "meta = \"//content//example-abf-files//mcc-settings_con.json\"\n",
        "root_path = [\"//content//example-abf-files//Example Files\"]\n",
        "\n",
        "for path in root_path:\n",
        "        print(path)\n",
        "        for r, celldir, f in os.walk(path):\n",
        "              \n",
        "              for c in celldir: ##Walks through each folder (cell folder) in the root folder\n",
        "\n",
        "                  c = os.path.join(r, c) ##loads the subdirectory path\n",
        "                  ls = os.listdir(c) ##Lists the files in the subdir\n",
        "                  \n",
        "                  abf_pres = np.any(['.abf' in x for x in ls]) #Looks for the presence of at least one abf file in the folder (does not check subfolders)\n",
        "                  if abf_pres:\n",
        "                        if bmeta == True: ##If the user provided an additonal json file, we copy that into the subfolder\n",
        "                            shutil.copy(meta,c) \n",
        "                            \n",
        "                        print(f\"Converting {c}\")\n",
        "                        nwb_r = convert(c,\n",
        "                                overwrite=True,\n",
        "                                fileType='.abf',\n",
        "                                outputMetadata=False,\n",
        "                                outputFeedbackChannel=False,\n",
        "                                multipleGroupsPerFile=True,\n",
        "                                compression=True)\n",
        "                        file = nwb_r\n",
        "                        with h5py.File(file,  \"r\") as nwb:\n",
        "                          print(F\"gain: {nwb['acquisition']['index_00']['gain'][()]}\")\n",
        "                          print(F\"sess desc: {nwb['session_description'][()]}\")\n",
        "                        confirm_metadata(nwb_r,meta) ##Call confirm metadata to overwrite the data\n",
        "                        with h5py.File(file,  \"r\") as nwb:\n",
        "                          print(F\"gain: {nwb['acquisition']['index_00']['gain'][()]}\")\n",
        "                          print(F\"sess desc: {nwb['session_description'][()]}\")\n",
        "                        os.remove(os.path.join(c,os.path.basename(meta)))\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "//content//example-abf-files//Example Files\n",
            "Converting //content//example-abf-files//Example Files/Cell 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipfx/x_to_nwb/ABFConverter.py:135: UserWarning: Could not find the JSON file /content/example-abf-files/Example Files/Cell 1/19904021.json with settings.\n",
            "  warnings.warn(f\"Could not find the JSON file {settings} with settings.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipfx/x_to_nwb/ABFConverter.py:135: UserWarning: Could not find the JSON file /content/example-abf-files/Example Files/Cell 1/19904023.json with settings.\n",
            "  warnings.warn(f\"Could not find the JSON file {settings} with settings.\")\n",
            "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
            "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
            "/usr/local/lib/python3.6/dist-packages/pynwb/file.py:619: UserWarning: Date is missing timezone information. Updating to local timezone.\n",
            "  warn(\"Date is missing timezone information. Updating to local timezone.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipfx/x_to_nwb/ABFConverter.py:445: UserWarning: Could not find the scale factor for the stimset Monkey Gap free, using 1.0 as fallback.\n",
            "  warnings.warn(f\"Could not find the scale factor for the stimset {stimset}, using 1.0 as fallback.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipfx/x_to_nwb/ABFConverter.py:445: UserWarning: Could not find the scale factor for the stimset Monkey_1000 ms step, using 1.0 as fallback.\n",
            "  warnings.warn(f\"Could not find the scale factor for the stimset {stimset}, using 1.0 as fallback.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipfx/x_to_nwb/ABFConverter.py:470: UserWarning: Could not find settings for amplifier unknown of channel IN 0.\n",
            "  warnings.warn(f\"Could not find settings for amplifier {amplifier} of channel {adcName}.\")\n",
            "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
            "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
            "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
            "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "gain: 1.0\n",
            "sess desc: PLACEHOLDER\n",
            "gain: 555.0\n",
            "sess desc: TEST DESC\n",
            "Converting //content//example-abf-files//Example Files/Cell 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipfx/x_to_nwb/ABFConverter.py:445: UserWarning: Could not find the scale factor for the stimset Monkey_3 ms step, using 1.0 as fallback.\n",
            "  warnings.warn(f\"Could not find the scale factor for the stimset {stimset}, using 1.0 as fallback.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipfx/x_to_nwb/ABFConverter.py:466: UserWarning: Stored clamp mode 0 does not match requested clamp mode 1 of channel IN 0.\n",
            "  warnings.warn(f\"Stored clamp mode {settings['GetMode']} does not match requested \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "gain: 1.0\n",
            "sess desc: PLACEHOLDER\n",
            "gain: 555.0\n",
            "sess desc: TEST DESC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5IH7P6wjHA1"
      },
      "source": [
        "In principle we convert and overwrite in the same go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8p-g535jP7_"
      },
      "source": [
        "## (optional) Removing Sweeps that fail QC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo7evgz0jUe6"
      },
      "source": [
        "Ideally, we could remove sweeps from the ABF files. However Removing sweeps from an ABF also removes ALL stim info from the ABF file.  \n",
        "To skirt around this, we can delete the the sweeps using H5py, this means that the remaining sweep retain the stim info. \n",
        "NOTE: Deleting data from nwb's seems to be highly discouraged by the pyNWB team, so its not in best practice to do this. pyNWB contains no way to remove data in house."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYYyRd8Aj8Px"
      },
      "source": [
        "def remove_sweeps(nwb_file, qcsweeps):\n",
        "  qcsweeps = np.asarray(qcsweeps) #If given a list convert to array\n",
        "  file_path = nwb_file\n",
        "  print(f\"QC'ing {file_path}\")\n",
        "  with h5py.File(file_path,  \"r\") as f:\n",
        "            item = f['acquisition']\n",
        "            sweeps = item.keys()\n",
        "            print(sweeps)\n",
        "  qc_names = []\n",
        "  for x in qcsweeps:\n",
        "          if x < 10:\n",
        "              qc_names.append(f\"index_0{x}\") ##This is for if the file has under 100 sweeps, otherwise the names will be something like, index_00X\n",
        "          else:\n",
        "              qc_names.append(f\"index_{x}\")\n",
        "  print(qc_names)\n",
        "  with h5py.File(file_path,  \"a\") as f:\n",
        "        item = f['acquisition'] ##Delete the response recording\n",
        "        for p in qc_names:\n",
        "              try:\n",
        "                  del item[p] #For whatever reason these deletes try to do it twice ignore the second error message\n",
        "                  print(f'deleted {p}')\n",
        "              except:\n",
        "                  print(f'{p} delete fail')\n",
        "        item = f['stimulus'] #next delete the stimset\n",
        "        for p in qc_names:\n",
        "              try:\n",
        "                  del item[p]\n",
        "                  print(f'deleted {p}')\n",
        "              except:\n",
        "                  print(f'{p} delete fail')\n",
        "        print(item.keys())\n",
        "        item = f['general']['intracellular_ephys']['sweep_table'] #next delete the references in the sweep table, or else the nwbs may break analysis\n",
        "        ## Since IPFX may go looking for sweeps that are absent\n",
        "        for key, value in item.items():\n",
        "              array = value[()]\n",
        "              ind = np.arange(0, len(array))\n",
        "              \n",
        "              bool_mask = np.in1d(ind,qcsweeps, invert=True)\n",
        "              new_data = array[bool_mask]\n",
        "              try:\n",
        "                del item[key]\n",
        "                item[key] = new_data\n",
        "                print(f'deleted and rewrote {key}')\n",
        "              except: \n",
        "                print(f'{key} delete fail')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJmEhae2sXdh"
      },
      "source": [
        "The function trys to delete sweeps that you pass in. However I may have missed something. \n",
        "Below shows an example of how utilize the function. It calls and confirms the sweeps are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPq4bsbmmB0L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "ad37f0eb-dbea-4354-d37f-c97115ec5a44"
      },
      "source": [
        "sweeps = [2,3]\n",
        "remove_sweeps(nwb_r, sweeps)\n",
        "with h5py.File(nwb_r,  \"r\") as f:\n",
        "            item = f['acquisition']\n",
        "            sweeps = item.keys()\n",
        "\n",
        "            print(sweeps) ##Confirm deletion\n",
        "            print(f['general']['intracellular_ephys']['sweep_table']['id'][()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QC'ing /content/example-abf-files/Example Files/Cell 2/../Cell 2.nwb\n",
            "<KeysViewHDF5 ['index_00', 'index_01', 'index_02', 'index_03', 'index_04', 'index_05', 'index_06', 'index_07', 'index_08', 'index_09', 'index_10', 'index_11', 'index_12', 'index_13', 'index_14', 'index_15', 'index_16', 'index_17', 'index_18', 'index_19', 'index_20', 'index_21', 'index_22', 'index_23', 'index_24', 'index_25', 'index_26', 'index_27', 'index_28', 'index_29', 'index_30', 'index_31', 'index_32', 'index_33', 'index_34', 'index_35', 'index_36', 'index_37', 'index_38', 'index_39', 'index_40', 'index_41', 'index_42', 'index_43', 'index_44', 'index_45', 'index_46', 'index_47', 'index_48', 'index_49', 'index_50', 'index_51', 'index_52', 'index_53', 'index_54', 'index_55', 'index_56', 'index_57', 'index_58', 'index_59', 'index_60', 'index_61', 'index_62', 'index_63', 'index_64', 'index_65', 'index_66', 'index_67', 'index_68', 'index_69', 'index_70']>\n",
            "['index_02', 'index_03']\n",
            "deleted index_02\n",
            "deleted index_03\n",
            "index_02 delete fail\n",
            "index_03 delete fail\n",
            "<KeysViewHDF5 ['presentation', 'templates']>\n",
            "deleted and rewrote id\n",
            "deleted and rewrote series\n",
            "deleted and rewrote series_index\n",
            "deleted and rewrote sweep_number\n",
            "<KeysViewHDF5 ['index_00', 'index_01', 'index_04', 'index_05', 'index_06', 'index_07', 'index_08', 'index_09', 'index_10', 'index_11', 'index_12', 'index_13', 'index_14', 'index_15', 'index_16', 'index_17', 'index_18', 'index_19', 'index_20', 'index_21', 'index_22', 'index_23', 'index_24', 'index_25', 'index_26', 'index_27', 'index_28', 'index_29', 'index_30', 'index_31', 'index_32', 'index_33', 'index_34', 'index_35', 'index_36', 'index_37', 'index_38', 'index_39', 'index_40', 'index_41', 'index_42', 'index_43', 'index_44', 'index_45', 'index_46', 'index_47', 'index_48', 'index_49', 'index_50', 'index_51', 'index_52', 'index_53', 'index_54', 'index_55', 'index_56', 'index_57', 'index_58', 'index_59', 'index_60', 'index_61', 'index_62', 'index_63', 'index_64', 'index_65', 'index_66', 'index_67', 'index_68', 'index_69', 'index_70']>\n",
            "[  0   1   4 ... 137 138 139]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSdV4zWRtvL7"
      },
      "source": [
        "Ideally, this is used in bulk by utilizing a dataframe that pairs cell_files with sweeps that fail QC. A rough implimentation of that is [here](https://github.com/smestern/ipfx/blob/master/ipfx/bin/run_sweep_QC.py)"
      ]
    }
  ]
}